# KE-BERT

**KE-BERT**  (**K**nowledge-**E**nhanced **B**ERT) is a knowledge-enhanced pre-training framework. It was proposed in my thesis. I introduce supervised learning factual knowledge into representations as a means of enhancing representations semantics. Experimental results show that my method improves on conventional contextual word representations and achieves good results on GLUE benchmark, CoNLL 2003, and TACRED dataset. This work will be presented at international conference, and I will provide method details and pretraining code in this repository in the near future.


## Released Model

The model can be downloaded through the link [https://drive.google.com/drive/folders/1YwgCRSq5PO31a7SSxGP4EJtQyKVOaR4f?usp=sharing](https://drive.google.com/drive/folders/1YwgCRSq5PO31a7SSxGP4EJtQyKVOaR4f?usp=sharing)

## Citation

If you use KE-BERT in your work, please cite the

[original paper]:

  
```
@article{tang2022kebert,
  title={KE-BERT: Pre-training of Knowledge-Enhanced Word Representation For Language Understanding},
  author={Tang, Yu-Siou and Wu, Chung-Hsien},
  year={2022}
}
```

## Contact Info

 Please submit a GitHub issue or send an e-mail to Yu-Siou Tang

(`yusiou.tang@gmail.com`) for help or issues using KE-BERT.




